# Unit Test Generation - Masters

## Links:

- https://dl.acm.org/doi/pdf/10.1145/3511430.3511433 (Human-based Test Design versus Automated Test Generation: A Literature Review and Meta-Analysis) [1]
- https://ieeexplore.ieee.org/document/8816768 (On the Effectiveness of Manual and Automatic Unit Test Generation: Ten Years Later) [2]
- https://ieeexplore.ieee.org/document/8367053 (How Do Automatically Generated Unit Tests Influence Software Maintenance?) [3]
- https://dl.acm.org/doi/10.1145/2771783.2771801 (Automated unit test generation during software development: a controlled experiment and think-aloud observations) [4]
- https://ieeexplore.ieee.org/document/7372009 (Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges (T)) [5]
- https://pythonistaplanet.com/difference-between-statically-and-dynamically-typed-languages/ [6]
- https://arxiv.org/abs/2111.05003 (An Empirical Study of Automated Unit Test Generation for Python) [7]
- https://ieeexplore.ieee.org/abstract/document/5954405 (Search-Based Software Testing: Past, Present and Future) [8]
- https://www.informs.org/Publications/OR-MS-Tomorrow/Metaheuristics-in-Optimization-Algorithmic-Perspective [9]
- https://arxiv.org/abs/2110.13575 (Automated Support for Unit Test Generation A Tutorial Book Chapter) [10]
- https://www.sciencedirect.com/science/article/abs/pii/S0950584901001896 (Search-based software engineering) [11]
- https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1701 (Choosing The Fitness Function for the Job: Automated Generation of Test Suites that Detect Real Faults) [12]
- https://www.hindawi.com/journals/jcnc/2019/7983583/ (The Characteristics of Metaheuristic Method in Selection of Path Pairs on Multicriteria Ad Hoc Networks) [13]
- https://www.baeldung.com/cs/heuristics-vs-meta-heuristics-vs-probabilistic-algorithms [14]
- https://www.researchgate.net/publication/220516273_Search-based_software_test_data_generation_a_survey_Research_Articles (Search-based software test data generation: a survey) [15]
- https://www.mygreatlearning.com/blog/an-introduction-to-hill-climbing-algorithm/ [16]
- https://link.springer.com/article/10.1007/s11042-020-10139-6 (A review on genetic algorithm: past, present, and future) [17]
- https://ieeexplore.ieee.org/document/488968 (Particle swarm optimization) [18]
- https://www.researchgate.net/publication/261741691_Genetic_Algorithms_Data_Structures_Evolution_Programs_by_Z_Michalewicz (Genetic Algorithms + Data Structures = Evolution Programs) [19]
- https://ieeexplore.ieee.org/document/4129846 (Ant colony optimization) [20]
- https://warin.ca/ressources/books/2015_Book_IntroductionToEvolutionaryComp.pdf (Introduction to Evolutionary Computing) [21]
- https://ia800200.us.archive.org/29/items/2008IntroductionToGeneticAlgorithmsS.N.Sivanandam/2008%20-%20Introduction%20to%20Genetic%20Algorithms%20%28S.%20N.%20Sivanandam%29.pdf (Introduction to Genetic Algorithms) [22]
- https://garph.co.uk/ijarie/mar2013/1.pdf (Encoding schemes in genetic algorithm) [23]
- https://www.baeldung.com/cs/genetic-algorithms-roulette-selection [24]
- https://www.researchgate.net/publication/257720048_Genetic_Algorithm_for_the_History_Matching_Problem (Genetic Algorithm for the History Matching Problem) [25]
- https://www.researchgate.net/publication/259461147_Selection_Methods_for_Genetic_Algorithms (Selection Methods for Genetic Algorithms) [26]
- https://www.sciencedirect.com/science/article/abs/pii/B9780080506845500082 (A Comparative Analysis of Selection Schemes Used in Genetic Algorithms) [27]
- https://link.springer.com/chapter/10.1007/978-3-642-16493-4_19 (A Review of Tournament Selection in Genetic Programming) [28]
- https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_parent_selection.htm [29]
- https://www.researchgate.net/publication/220741401_An_analysis_of_multi-sampled_issue_and_no-replacement_tournament_selection (An analysis of multi-sampled issue and no-replacement tournament selection) [30]
- https://www.researchgate.net/publication/321065158_Termination_Criteria_in_Evolutionary_Algorithms_A_Survey (Termination Criteria in Evolutionary Algorithms: A Survey) [31]
- https://www.researchgate.net/publication/262219658_Variance_as_a_Stopping_Criterion_for_Genetic_Algorithms_with_Elitist_Model (Variance as a Stopping Criterion for Genetic Algorithms with Elitist Model) [32]
- https://www.researchgate.net/publication/220701522_An_Emperical_Study_on_GAs_Without_Parameters (An empirical study on GAs “without parameters”) [33]
- https://www.sciencedirect.com/science/article/pii/S2212671613000449 (APOGA: An Adaptive Population Pool Size Based Genetic Algorithm) [34]
- https://www.researchgate.net/publication/3623609_Sizing_the_population_with_respect_to_the_local_progress_in_1l-evolution_strategies-a_theoretical_analysis (The choice of the offspring population size in the (1,λ) evolutionary algorithm) [35]
- https://direct.mit.edu/evco/article-abstract/13/4/413/1223/On-the-Choice-of-the-Offspring-Population-Size-in?redirectedFrom=fulltext (On the Choice of the Offspring Population Size in Evolutionary Algorithms) [36]
- https://ieeexplore.ieee.org/document/350039 (GAVaPS - a Genetic Algorithm with Varying Population Size) [37]
- https://link.springer.com/chapter/10.1007/3-540-45356-3_31 (An empirical study on GAs “without parameters”) [38]
- https://www.researchgate.net/publication/220702069_Evolutionary_Algorithms_with_On-the-Fly_Population_Size_Adjustment (Evolutionary Algorithms with on-the-fly Population Size Adjustment) [39]
- https://www.sciencedirect.com/science/article/pii/S2212671613000449 (APOGA: An Adaptive Population Pool Size Based Genetic Algorithm) [40]
- https://www.researchgate.net/publication/220701819_Is_Self-adaptation_of_Selection_Pressure_and_Population_Size_Possible_-_A_Case_Study (Is Self-adaptation of Selection Pressure and Population Size Possible? – A Case Study) [41]
- https://www.researchgate.net/publication/229699048_Adaptive_selection_routine_for_evolutionary_algorithms (Adaptive selection routine for evolutionary algorithms) [42]
- https://www.researchgate.net/publication/259009318_Parent_Selection_Operators_for_Genetic_Algorithms (Parent Selection Operators for Genetic Algorithms) [43]
- https://www.mdpi.com/2078-2489/10/12/390 (Choosing Mutation and Crossover Ratios for Genetic Algorithms — A Review with a New Dynamic Approach) [44]
- https://www.researchgate.net/publication/281532313_Adaptive_Operator_Selection_for_Optimization (Adaptive Operator Selection for Optimization) [45]
- https://ieeexplore.ieee.org/document/771166 (Parameter Control in Evolutionary Algorithms) [46]
- https://ieeexplore.ieee.org/document/4075583 (Optimization of Control Parameters for Genetic Algorithms) [47]
- https://www.researchgate.net/publication/267027822_Fuzzy_adaptation_of_crossover_and_mutation_rates_in_genetic_algorithms_based_on_population_performance (Fuzzy adaptation of crossover and mutation rates in genetic algorithms based on population performance) [48]
- https://link.springer.com/chapter/10.1007/3-540-61286-6_141 (Intelligent mutation rate control in canonical genetic algorithms) [49]
- https://link.springer.com/chapter/10.1007/3-540-61286-6_141 (Intelligent Mutation Rate Control in Canonical Genetic Algorithms ) [50]
- https://www.researchgate.net/publication/220742566_Self-adaptive_mutation_rates_in_genetic_algorithm_for_inverse_design_of_cellular_automata (Self-Adaptive Mutation Rates in Genetic Algorithm for Inverse Design of Cellular Automata) [51]

## Automated and Manual Testing

The generation of unit tests reveals as it being one of the most important aspects of software development. Creating unit tests to evaluate the behavior of the different units within the system is a vital procedure to ensure the system works as intended. However, one must take into account how this procedure can be done. 

Two different methods to create unit tests are known. They can be generated using human effort, in a manual manner or can be generated automatically through the use of state-of-the-art unit test generation algorithms. This last sentence brings forth a relevant question, "Which of them should be used over the other?". This work does not elaborate on this question, however certain points can be said about their differences.

- Scaling problems:
    - Manual generation can be an exhaustive and a time-consuming process. It scales with the size of the project. Projects with a great amount of parts require even more unit tests to be created. Manually creating these tests can hinder the development speed of the software;
    - Automated generation, being an automated process, can help reduce the time needed to perform this process. A manual procedure tends to be more time-consuming than a automated one [1].

- Coverage values and mutation scores:
    - According to studies that evaluate manual and automated test generation coverage values ([1],[2],[3]), automated generation of unit tests proves to have a higher capability of achieving better coverage values than the manual approach. The ability to identify mutants in unit tests (identification of allocated defects) is generally better in unit tests generated automatically [2].

- Fault detection:
    - While automated generation proves to be superior in terms of code coverage and mutation scores, there is less consistency in favor of automated methods when it comes to fault detection [2],[5]. Manual generation seems to perform comparatively better in this regard.

## Dynamic vs Static Languages

One important point to address is to determine if the programming language used in the generation is statically or dynamically typed. This is vital as each one possess different characteristics that can either be an advantage or disadvantage for the automated generation of unit tests. 

In order to further contextualize this topic, a definition about type checking is presented. Type checking is a process that consists in verifying/checking the type of a construct (list, array, variable among others) and its usage [6]. This process can happen during compile-time (static checking) or during run-time (dynamic checking) of a program. A programming language can be defined as a **statically typed language if the type checks happen at compile time** and as a **dynamically typed language if the type checks happen at run-time**. Moreover, in statically typed languages the variable and data types are known before run time however in dynamically typed languages the variable and data types are only known in run time. This can cause a larger execution time for programs as the types are determined during the execution process.

Automated test generation is affected if the used language is statically or dynamically typed. The process is negatively affected it the chosen language is dynamic as the latter requires the types to be specified previously in order provide enough type context for generation of tests. When the generation process is under execution, the functions to be created do not know the required types for their parameters [7].

This work delves this important characteristic by applying automated generation of unit tests in a statically typed language (Java) and a dynamically typed language (Python) in order to show how impactful can type information be for the generation of tests.

## Search-Based Test Generation

Test creation, as mentioned previously, can be a rather tedious/intensive task to be executed manually. It requires selecting sequences of program input as well as oracles to evaluate if the given test is executing as expected. Automating this process can effectively reduce this intensive and time-consuming task being this possible by applying a automated test creation technique called Search-Based Test Generation.

Search-Based Test Generation consists in a technique to seek the best test suites for a given SUT, with the use of metaheuristic search algorithms within a restricted time limit. When generating test cases certain goals must be met such as achieving the best code coverage, triggering assertions within the SUT or even finding possible faults. The principal objective is to generate tests to meet certain goals as this can be considered as a search problem all together [11]. A search is being made to find the best possible solutions (test suites) that achieve specific goals. 

The generation is obtained by executing a segment of code, being this process guided by cost functions, in other terms "fitness functions", to achieve the best test data possible [8]. This process is also aided with the application of metaheuristic algorithms, which provide a solution for optimization problems found within unit test generation. 

### Fitness Functions

Fitness functions play a crucial role in search-based test generation. These functions are responsible for determining the quality of a given test case. They evaluate the generated test cases, suggest improvements, and indicate how close they are to achieving a desired goal [10]. Fitness functions must adhere to the following requirements:

- Return **continuous scores** as to offer better feedback for the metaheuristic algorithms;
- Return **only numeric values** in order to properly evaluate the generation of test cases each time;
- Indication of how close the generation was to being optimal. It should not indicate quality but a distance to optimal quality [10]. 

Fitness functions serve as a guiding principle for the metaheuristic algorithms, as they take the attributed score for the generated tests to reformulate them for the next iteration of the generation process [12].


#### Desired Goals

As previously mentioned, the fitness function gives a scores for each generation attempt which can detail how close the generated tests are to achieving a specific goal. What is this goal concretely? When generating tests, one must ensure the tests are properly adequate. It's not entirely sure if a generated test is always relevant for the content in the SUT. A adequacy criteria must be deployed to ensure the test represents the necessary testing rationale for the SUT. 

Common methods to measure adequacy are coverage of structural elements of the software including executions of statements, flow of branches and boolean conditional statements. These were already previously mentioned in section "Code Coverage Metrics", where different metrics for code coverage - such as branch, statement or conditional coverage - can prove to be a good adequacy criteria for the fitness function. For example, a fitness function can return a score for a generated test given its fullfillment of a adequacy criteria (coverage metric). This can guide the generation process providing enough detail to help differentiate candidate solutions and always aim for optimal candidate solutions [12].

### Metaheuristic algorithms

Metaheuristic algorithms implement a search procedure to find the best solution possible within a search space while also obeying a restrict time limit or search budget. They are also a powerful aid to ensure the search of a near-optimal solution with incomplete ou imperfect information with the available resources [9]. The objective - behind these algorithms - is to find new strategies to resolve a problem as they use heuristics for that matter. 

The major distinction between a metaheuristic and a heuristic is that the latter needs to be tailored to a specific problem. A metaheuristic is a general algorithm which can be used in different types of problems being an problem-independent algorithm [14] being generally associated to a black-box technique. In context of search-based test generation this can be exemplified regarding coverage metrics. The same heuristic cannot be used for two different coverage problems as the heuristic needs to be tailored to a specific coverage problem, however the use of a metaheuristic proves to be better as it can take a problem in a general context, i.e., does not need to be specified for a specific problem.

Applying metaheuristic algorithms in conjunction with fitness functions can aid in the stable and efficient search for near-optimal solutions in the context of search-based test generation. The process of generating test cases can be seen as an optimization problem, where the objective is to search within a space of possible inputs and improve the quality of the solutions over time. The use of fitness functions in this optimization process is crucial. Fitness functions assign scores to individual test cases based on their adherence to specific adequacy criteria, such as coverage metrics or other relevant quality measures. These scores guide the metaheuristic algorithms to explore the search space and find better, more optimized solutions with each iteration.

By leveraging the power of metaheuristic algorithms and fitness functions, search-based test generation aims to continuously improve the quality of generated test cases within the constraints of a given search budget. The process involves iteratively evaluating and refining the solutions based on their fitness scores, ultimately leading to test suites that better achieve a specific goal.

#### Hill Climber

Hill Climber is a metaheuristic algorithm that focuses in searching a local optimal solution within a search space. The algorithm starts with a single initial solution chosen randomly and makes continuous searches around its neighbourhood to find better solutions. If a better solution is found - within that neighbourhood - then the current solution is replaced by the better solution. This last process is repeated continously until the search budget is reached or if no improved neighbours can be found. It is considered as a single-solution based metaheuristic [17].

This algorithm is essentially a local search algorithm. It tries to find a local optima, i.e., trying to find the local maximum within the search space, which represents the state who maximizes the value of a objective function (fitness function). This however is a limitation to the algorithm as it may not achieve the most desired score which is the global maximum, which represents the highest objective score. 

During the search, a local maximum can be found and limit the search as it is not possible to find a better neighbour within that search scope (further "climbing" is not made) because the neighbours values are worse than the current found solution [16]. Another problem is that the search may lead to search spaces where the solution cannot be improved any further because of the neighbours sharing the same objective score, i.e., there is a high quantity of values that are equal to the current solution. All these equal values are designated as "flat" local maximums. A problem that also associated to these "flat" values is the **shoulder problem**. This identifies a range of equal objective values which prevents founding the best solution within that search space. The regions where all neighbours have the same values are called **plateau** regions.

Another major limitation behind this algorithm lies upon the starting solution. If a bad solution is chosen to start the search, there is a higher chance that the search scope can be very limited not providing enough good solutions. In this case, the best solution can be found almost imediately which is a bad indicator for the search as it means the starting solution was not a good choice and the peak - within the search space of the starting solution - was almost instantly achieved.

![](https://hackmd.io/_uploads/BJkFJBndn.png) [16]


#### Genetic Algorithm

Not all metaheuristic algorithms delve onto the local search scope, as some of them also perform a global search and consider a wide set of candidate solutions within the search space. These algorithms are called "Population-based metaheuristics" and can maintain diversity in the population (set of solutions) while also prevent the search to be stuck in a local optima [17]. A few example of these algorithms are swarm optimization (PSO) [18], genetic algorithm (GA) [19] and ant colony optimization (ACO) [20]. 

For this work, genetic algorithm was implemented and as such, a much needed contextualization about evolutionary algorithms must be presented. An evolutionary algorithm is an algorithm who uses a search strategy to evolve candidate solutions using genetics and natural selections inspired operators [15]. A more detailed definition can be defined as: "**given a population of individuals within some environment that has limited resources, competition for those resources causes natural selection (survival of the fittest)**" mentioned by A.E. Eiben and J.E. Smith [21]. 

Evolutionary algorithms (EAs) are inspired by the Darwinian Evolution, where the solutions are identified as individual organisms in a population (set of solutions). Within that population, each individual is tested for fitness (how well they can resolve a problem) and a set of them are selected for reproduction, which includes crossover and mutation operators. Genetic algorithm is a class of the evolutionary algorithms and as such also employs bio-inspired operators for its generation of solutions.

##### Exploration vs Exploitation

The search procedure behind these algorithms can be divided in exploration and exploitation phases. Initially the search must focus in exploring  different regions of the search space to find feasible solutions being this identified as a **exploration phase**. Additionally, when good solutions are found, more of these solutions must be sought after around their vicinity, i.e., more search should be made in the region where the good solutions where found being this a **exploitation phase**. Howewer, neither of these phases must have a higher impact than other because it can lead to undesired outcomes. A higher exploration can hinder the performance of the algorithm as it can induce an inefficient search (unnecessary search can be made) and a higher exploitation can lead to **premature convergence** where a good solution if found too quickly and the search gets stuck in a region where no improvement can be made (despite possible better solutions can exist) not enabling a proper search of the search space. These two phases must be complemented between each other and balanced as it can prevent the algorithm to be stuck in local optima and provide a higher chance for a global optima to be found within the search space.

:::warning
**(VER PAPER E ACRESCENTAR ESTA DEFINIÇÃO)**
![](https://hackmd.io/_uploads/r1VT1Ck23.png) [32]
:::

##### Natural phenomena

A genetic algorithm is a optimization algorithm that uses bio-inspired operators for its generation of solutions. Being a population-based search algorithm, it employs the concept of survival of the fittest [19] where only the individuals with the highest fitness scores survive. New populations are formed iteratively through genetic operators applied on the individuals of the present population. These genetic operators will alter the fittest individuals of the population and will apply crossover and mutation operations to them in order to generate even better individuals. This process is repeated until a certain budget is met and the population will contain the best generated individuals until then. The application of GA, with biological terms, can be explained in a simple step scheme:

1. **Population**: a set of $n$ chromossomes, where each one of them represents one individual, is initialized randomly;
2. **Selection**: operation that selects the fittest individuals of the current population. the $t$ chosen individuals (chromossomes) are selected through their fitness score value (the $t$ highest fitness score values are chosen);
3. **Recombination**: the chosen chromossomes undergo crossover operations, i.e., they exchange information between each other according the type of crossover specified. The newly created chromossomes are now referred to as "offsprings";
4. **Mutation**: according to the type of mutation operator, small changes will be made to the offsprings in order to add more evolution/information;
5. **Adding offsprings**: after the mutation process, the offsprings are placed in a new population replacing the old population entirely. After this step the search will continue in the new population;
6. **Repetition of step 2 - 5**: the steps 2 through 5 are repeated until a certain limit, budget or objective is reached within the generation process.

###### Individuals representation

In natural evolution terms, an individual or chromossome components are called genes, the values for each component are referred to as alleles and their position within the sequence of the chromossome are called locus. In this work, these biological terms will be referred interchangeably between future sections.

Before applying a genetic algorithm to a domain problem is necessary to decide how the components of the domain (more accurately the solutions) will be represented as. In this case, the chromossomes (solutions) must be stored and manipulated in way that a computer can understand it. The objects that encompass possible solutions within the problem context are called as **phenotypes** and their respective enconding form is called **genotypes**. This first step is called **representation** where mapping from phenotypes onto a set of genotypes is necessary for proper representation of the solutions as the search is conducted in the genotype space [21].

The solutions can be encoded in a fixed-length bit string [22]. Next, a type of enconding is necessary, where for problems involving genetic algorithms, a binary enconding is often recommended [17]. However, it's worth noting that there are other encoding schemes that can be used in genetic algorithms, including octal, hexadecimal, permutation, value-based, and tree encodings [23]. These are the standard encoding schemes used for genetic algorithms, however encoding schemes are dependent on the problem domain and not all the mentioned encoding schemes may work for more specific problems.

In binary encoding scheme, a chromossome is represented as a string of values 1 or 0 (binary string) and in this enconding each bit represents the characteristics of the solution. Using a binary enconding provides a faster implementation of crossover and mutation operators [17] as each allele is one of two values providing simpler operations. 

###### Selection types

After a proper representation of the individuals, a few of them must be selected to start reproduction which include crossover and mutation operations. This selection is made according to the fitness score value of the individuals - in a population - according to how well they resolve a defined problem. In this process, $n$ fittest chromossomes will be chosen to act as **parents** for the next stage of the algorithm (reproduction phase). This selection process is typically randomized, with the probability of selection depending on the fitness of the individuals [22]. The higher the fitness of the individual, higher the chance he will be picked as a parent for reproduction. 

There is two major categories of methods for using fitness in the selection operation:
- **Deterministic methods**: these methods involve selecting $n$-fittest individuals of a population. Only the $n$ individuals with the highest fitness scores are chosen for reproduction. This can lead the population to reach a local maximum which consecutively stops the evolution process;
- **Stochastic methods**: as one of the major objectives of genetic algorithms is to provide a global search within the population, stochastic methods are often recommend to avoid the population to get stuck in local optima. These methods select individuals randomly with a probability according to fitness values [22]. 

Selection types:

- **Random selection**
    - The chromossomes are selected randomly, within the population, without any analysis on the fitness scores of the individuals. This in on itself is a huge problem for proper generation of unit tests that are representative of the problem context as random selection of the parent chromossomes generally does not provide an adequate generation in the later stages;
- **Roulette Wheel selection**
    - One of the traditional genetic algorithm techniques where an individual is selected, from a population, with a probability proportional to the fitness score [22] being it considered as a stochastic method. 
    - The roulette wheel selection can be described, in simpler terms, as a wheel divided in sequential spaces where each of these spaces or slots are proportional to the fitness score of each individual within the population. 
    - This technique promotes a linear search through the wheel (population) defining a target value to be achieved. This target value corresponds to a random proportion of the sum of the fitness values of all individuals in the population. The population is searched until the target value is reached. This however constitutes a potential problem.
        - It's not guaranteed that fit individuals are selected despite having bigger chances (their fitness scores are higher). If a fit individual does not exceed the target value, there might be a chance that the next individual in line can exceed the target value while being weaker that the previous one. In this technique the population must not be sorted by fitness scores in order to prevent bias in the selection [22].
        - Another problem associated to this technique is that more predominant individuals (with high fitness scores) will introduce bias to the initial search as they occupy a major portion of the wheel. This leads to premature convergence and loss of diversity as another individuals have low chances to be selected. This situation can hinder the search process as it almost limits the search to only one individual in the population being this almost compared to the situation of local optima in single-solution based metaheuristics.
        - **Selection probabilities** 
            - ![](https://hackmd.io/_uploads/Hko97tYYn.png) - total sum of probability of each chromossome (this **must be equal to one**) where $n$ represents the **number of chromossomes** and $p_{i}$ is the **probability of the $i$ chromossome**
            - ![](https://hackmd.io/_uploads/HyFrVKYYn.png) - $p_{x}$ probability value of a $x$ chromossome where the probability is the **quotient** between its **fitness score** $f_{x}$ and **sum of fitness scores of all chromossomes** in the population where $f_{i}$ represents the **fitness score of the $i$ chromossome** out of $n$ chromossomes in the population. For **maximization problems this is the formula to use**, for minimization problems $f_{i}$ would be $\frac{1}{f_{i}}$ instead

        ![](https://hackmd.io/_uploads/Sy-_jjHY3.png) [22]
        
        ![](https://hackmd.io/_uploads/Hy3iIYtFh.png) [25]
        
- This selection method has a $O(N^2)$ time complexity. A linear search is made through the wheel and with a population of size $n$ ($n$ spins) it has $O(n)$ just for the linear search for the whole wheel. As this method is repeated $n$ times an additional time complexity is added totalling $O(n^2)$ time complexity [27].


- **Rank selection**
    - A ranking operation is made for each chromossome within the population. Each individual receives its rank according to the fitness value that each one was. The worst individual will have rank 1 while the best individual will have rank $N$ whereas $N$ refers to the number of chromossomes within the population. 
    - In this technique, the individuals in a population are initially ordered by their fitness scores in ascending order of fitness. After the ordering, each respective individual is assigned a rank starting from the initial position of the ordered values. The rank is assigned from 1 (worst individual with the lowest fitness score) to $N$ (best individual with the highest fitness score). Then this technique, as already mentioned previously, determines the selection using the ranks and not fitness scores themselves. For this matter, a selection probability is determined using the ranks of each individual and population size. There is two ways to calculate this selection probability which are:
        - **Linear rank selection**: ![](https://hackmd.io/_uploads/ByfS-nqF2.png) whereas $i$ represents the $i^{th}$ individual in the population and $n$ represents the population size (total number of individuals). [26]

        - **Exponential rank selection**: ![](https://hackmd.io/_uploads/SJcIwRqFh.png) whereas $i$ represents the rank of the $i^{th}$ individual in the population and $c$ represents a normalization factor chosen so that the sum of the probabilities of all individuals equals to one.

![](https://hackmd.io/_uploads/r1b8iXp92.png) [22]

- This method has a time complexity of $O(n * log\ n)$. The technique is a two step process as it needs a sorting operation first before making the selection operation. Taking this into account, for the sorting operation, a time complexity of $O(n * log\ n)$ is needed (using standard sorting techniques), a propriate selection method varies its complexity between $O(n)$ and $O(n^2)$ [27]. According to these steps, the standard ranking selection method has a time complexity of $O(n * log\ n)$.


- **Tournament selection**
    - A variation of rank-based selection aproaches as it consists in a random selection of $k$ individuals from a population (with or without replacement). This is made to select the fittest individual out of the $k$ individuals selected in order to become a parent for the reproduction process. This process can be repeated $N$ times until the required number of parents are selected and it does not apply the sorting of fitness as the base rank selection method does [15]. Sorting is an operation that consumes a lot of time, being this critical in a population with million of individuals and as this method does not perform any sorting whatsoever, it contributes for a lower time complexity [28].
    - This technique vastly differs from the previous ones regarding the selection operation regarding the size of the population. As this method selects $k$ individuals from a population for the tournaments, a global knowledge of the population is not necessary [21]. However **the quantity of individuals in each tournament** (tournament size) is a **important parameter to evaluate**. The larger the tournament size, the greater are the chances to obtain better fit individuals, the smaller the tournament size the greater the chances to obtain less fit individuals. As the tournament size increases, higher the probability of high-fitness individuals to be selected and lower the probability of low-fitness individuals to be selected [21]. Standard used tournament sizes are 2, 4 and 7 [28].

    ![](https://hackmd.io/_uploads/H1el2W2c2.png) [29]

    - This method is the most popular selection technique used in genetic algorithms due to its time complexity $O(n)$ (linear time complexity). The random selection of a constant number of individuals out of a population of $n$ individuals while also comparing the chosen individuals between each other, done in constant time, all translate into a time complexity of $O(n)$ for this selection method.
    - The sampling from the population can be done with or without replacement. This is of great importance as it can bring some inconsistencies during the selection process as the same individual can be sampled from the population multiple times for the tournament, being this problem referred to as a multi-sampled issue [30]. The sampling issue of a tournament with replacement can be easily solved by not replacing any chosen individuals for the tournament, i.e., not returning individuals to the population that were chosen for the tournament until the parent is chosen.

![](https://hackmd.io/_uploads/r1Z2cmp53.png) [28]


###### Recombination operators

- After the parents are selected, the next phase of the genetic algorithm consists in exchanging information between the selected individuals in order to generate offsprings. This phase is designated as **recombination**, where a new individual is created from information of two or more parents [21]. This introduces genetic diversity as genes will be exchanged between individuals to create new chromossomes with different information from their parents. While the term "recombination" is more used to refer to exchange of information between parents, the term "crossover" can also be used for this phase.
- The recombination process is one of a probabilistic nature. This process is applied according to a crossover probability ($P_c$). This probability decides whether the crossover happens between two parents selected from the population. This characteristic does have some implications according to the selected probability value:
    - A higher probability does apply crossover operations between parents more often. However this can lead to a faster convergence to a optimal solution which, in certain situations, cannot cover a major portion of the individuals in the population resulting in a loss of diversity in the solutions;
    - A lower probability results in a lower rate of crossover operations, leading to the offspring being similar to their parents. Although this slower convergence may delay the attainment of a solution, it can also limit the acquisition of fitter individuals compared to the initial population. In terms of the crossover probability $P_c$, the likelihood of a crossover not occuring is  $1 - P_c$.

- There is a multitude of different crossover operators to apply in genetic algorithms for the recombination process. The standard crossover operators used in genetic algorithms applications are:

Crossover operators

- One-Point Crossover:
    - Exchange of genes between parents occurs after a specific position within the chromosome sequence. The selection of the split position is determined randomly, choosing one allele within the range from the first to the penultimate position in the chromosome sequence. In this type of crossover, the head and tail of one chromossome cannot be exchanged and if they contain good genetic information this can be a drawback [22]. In this operator, after the spliting position has been decided, the succeeding genes are exchanged.
    ![](https://hackmd.io/_uploads/H1f60aJon.png) [21]
    
- N-Point Crossover:
    - Similar to the one-point crossover operator however $n$ spliting points can be chosen to exchange the genes. The following image presents an example for $n = 2$. The genes situated between the spliting positions are exchanged. This type of crossover enables the passage of information from both the head and tail, of a chromossome sequence, to the offspring unlike the one-point crossover.
    ![](https://hackmd.io/_uploads/BkXMy01s3.png) [21]

- Uniform Crossover:
    - In this operator, each gene is treated independently, i.e, there is no exchange based on sequences of contiguous genes. This process makes a random choice to select the genes to exchange between parents. 
    - For the length of the chromossome sequence, a list of random values in a uniform distribution $[0,1]$ is made and a parameter $p$ is chosen to permit the exchange (usually 0.5). For each gene its corresponding value in the list will be compared to the parameter created. If the value is below the parameter $p$ the gene is inherited from the first parent. In the opposite situation the gene is inherited from the second parent. A second offspring is created according to the inverse mapping of the first offspring created by this process [21]. The following image shows the uniform crossover for the list $[0.3, 0.6, 0.1, 0.4, 0.8, 0.7, 0.3, 0.5, 0.3]$ and $p$ = 0.5. 

![](https://hackmd.io/_uploads/ryyhwgljn.png) [21]

- For different encoding schemes, different crossover variations need to be applied. For example for binary enconding schemes only one-point, n-point and uniform crossover can be applied whereas for a tree encoding scheme a subtree crossover is necessary. Another known crossover operations include shuffle, Precedence Preservative Crossover (PPX), ordered crossover, Partially Matched Crossover (PMX) and others [22].

###### Mutation operators

- Mutation is the next phase after the recombination process. In of itself, is a variation operator to add more diversity into the offsprings generated from the recombination by adding new information to them. Adding new information consists in altering a gene within the chromossome sequence in order to generate a individual with new information. 

- This operation prevents the genetic algorithm to enter in a local minimum as also recovers any genetic information lost from the recombination process. Mutation is viewed as a process that mantains the genetic diversity in a population [22] as it introduces new information in the population by randomly changing genes. 

- In a binary representation, a simple mutation consists in inverting values for each gene of the chromossome with a probability $p_m$. Usually this probability is about $1/L$ being $L$ the length of the chromossome that will undergo a mutation process. The bit flipping method is a popular mutation operator for binary representation. Important to note that given the implementation of the mutation process, the parameter that regulates the mutation can be a mutation probability, a mutation rate or a mutation step size [21].

- The mutation parameter is crucial to consider, as varying mutation rates can have a significant impact on the generation of fit individuals. A lower mutation rate is often recommended for populations who contain a high proportion of high fitness individuals as to not disrupt the quality of the population. Conversely, where only a small percentage of the population is composed of fit individuals, a higher mutation rate is recommended to introduce more diversity.

- Mutation operations are representation dependent (different encoding schemes have different ways to apply mutation).

Mutation operators:

- Flipping:
    - This mutation operator consists in changing a gene value to its inverse. This is typically associated with bit flipping changing values from within the interval of integer values $[0,1]$.
    - In this method, each gene has a probability $p_m$ for its value to be changed (bit flip of a gene). The number of values to be changed is not fixed, as it is a random choice in the interval of integer values $[1,L]$, where $L$ represents the length of the chromosome sequence undergoing mutation. On average $L\ *\ p_m$ genes will undergo changes within the chromossome sequence [21].

    ![](https://hackmd.io/_uploads/Sy7mvq1nh.png) [22]

- Interchanging:
    - Two random genes in the chromossome sequence are chosen and interchanged between them. Two random genes are chosen from the interval of integer values $[1,L]$ (the genes must be different from each other), occuring a interchange between those genes.
    
    ![](https://hackmd.io/_uploads/SyBZ9c132.png) [22]

- Reversing:
    - A random gene is chosen in the chromossome sequence and all the locus after it are reversed between each other. For the interval of integer values $[1,L]$, a random number $n$ is chosen and all the values in the interval of $(n,L]$ (whose values refer to the loci of the chromossome) have their corresponding genes reversed between each other.

    ![](https://hackmd.io/_uploads/H16j6913n.png) [22]
    
- Other methods that are widely used for mutation include the displacement mutation operator (DM), simple inversion operator (SIM), scramble mutation operator (SM) [17].


###### Termination Criteria

- After the mutation process occurs, the new generated individuals will be placed in a new population. This new population will undergo the same evolution process mentioned in the earlier sections comprising the selection, crossover and mutation methods. However, this evolution cannot go on forever as it would cost a great amount of computacional resources as well it would not be efficent for the generation process itself as a convergence point can be reached. If the algorithm reaches a point where the fittest individual is found and there is no margin for more evolution then the algorithm must stop generating individuals. This situation brings forth the question of "when can we stop the generation" and for this situation, stopping criterias must be defined for the algorithm.

- Stopping criterias enable the algorithm to achieve definitive outcomes while conserving computational resources and minimizing execution times. By carefully defining these criteria, the algorithm can strike a balance between thorough exploration and efficiency. Some standard stopping criterias used for evolutionary algorithms are listed below:
    -  **Maximum number of generations**: the algorithm continues the generation process until it reaches a maximum number of generations/iterations where the best seen solution, until then, is taken as the optimal one [32].
    -  **Time budget**: after a certain time limit has elapsed, the process of generation stops and the best seen solution is taken as the best one. This time can be measured as absolute time or CPU-time.
    -  **Stagnation of fitness score**: if the overall fitness score of the generation process does not improve for a specified number of generations, the execution stops and the best seen individual until then is obtained.
    -  **Maximum number of objective function evaluations**: the algorithm stops after reaching a maximum number of objective function evalutions. 
    -  **Best and Worst individual**: the execution stops when the best and worst objective value are less or equal than a given threshold $s \ge 0$.

###### Parameter Optimization

Parameter optimization is a rather crucial aspect involving the use of genetic algorithms. These parameters dictate if a genetic algorithm can provide a optimal or near-optimal solution to a given problem as they strongly affect the search process during the execution. The mentioned parameters are population size, selection probability, crossover and mutation rates. These parameters drive the search process of the genetic algorithm and reflect a major part in the **exploration vs exploitation** phases of the latter [45]. They can provide a stronger or weaker effect to each of the phases and guide the search to a optimal or a sub-optimal solution. In the following sections, each of these parameters and their respective impacts on the search process will be explained thoroughly. Additionally, for each of these parameters, implementations from different researches will be presented along their solutions for limitations behind the conventional and ad-hoc choices for the parameter values.

This topic is an important area of research in genetic algorithms and it has been known as "parameter setting". One of the first works that delved upon the importance of adjusting these is the work by Eiben, Hinterding and Michalewicz [46], where the authors classify two major forms of setting parameters values in genetic algorithms. These forms are **parameter tuning** and **parameter control**, where the first one consists in finding appropriate values for the parameters before the run of the algorithm keeping them fixed during the run and the second one consists in changing the parameter values during the run of the algorithm. For the latter, three different techniques can be applied:
- Deterministic parameter control
    - The parameter is adjusted according to a pre-determined rule. This rule alters the parameter value without using any guidance from the search process being an independent and deterministic method.

- Adaptive parameter control
    - Feedback is obtained from the search process during the run being it used to modify the parameter accordingly.

- Self-adaptive parameter control
    - The parameters are included in the chromossomes representation and undergo the same evolution process. In this way, this technique is often called "evolution of evolution" [45].

![](https://hackmd.io/_uploads/S11y1b8A3.png) [46]

Parameter tuning and parameter control despite having the same objective they differ largely in terms of potential benefits. A major drawback behind the parameter tuning process is the lack of an adaptive mechanism to guide the parameter optimization. The optimization in this method often relies on the manual modification of values after several runs of the genetic algorithm (parameter tuning based on experimentation). Trying all possible values is pratically impossible as trying all combinations is a "aimless" approach as well as being a time wasting process. Not even a vast majority of the values are guaranteed to be appropriate for a given search problem. The same can be said about the deterministic parameter control. Even a deterministic rule can determine the best guidance as the search progress is not being analysed [45]. This can hinder the search process guiding it to sub-optimal solutions that do not provide a good enough result for the given search problem. Adaptive and self-adaptive methods however, can correct these guidance flaws as taking account several aspects of the search process can enable a much more stable guidance to optimal solutions than one that follows a rule or its constantly altering parameter values in hopes of finding optimal or near-optimal solutions.

The following sections will introduce each parameter importance, its drawbacks when using standard or conventional values and studies that implement new techniques to further boost the eficiency of parameter values of a genetic algorithm.

- Population Size
    - A small population size can induce a quicker search for a good solution, however the chances associated with the search being stuck in a local optimum are much higher with a reduced population size despite requiring less computacional power.
    - A higher population size can prevent the search to be stuck in local optima situations. This gives more room for the search to find a global optima, however more computational resources are needed as more evaluations of individuals are made for bigger populations.
    - Given the problems according the sizes of the population an improvement must be sought after. By convention the population size must be a fixed value until the generation ends. However a better implementation would be the dynamic control of population size during the generation process. A lot of works have applied different methods to implement this dynamic variance of the population. A few of them will be explained more in detail than others depending on the relevance of aplication for this work. The investigated works about dynamic variance of population size are as follows:
        - An adaptive population scheme by Hansen, Gawelczyk and Ostermeier that consists in adapting the offspring population size for $(1,\lambda)$ evolution strategies according to the differences in fitness scores between the second fittest offspring and the parents [35].
        - Another method, employed by Jansen, De Jong and Wegener, were they implemented a scheme that enables the control of the offspring population size in a $(1+\lambda)$ evolutionary algorithms. Utilizes two major components such as the number of successes (offsprings with better fitness scores than parents) and the offspring population size $\lambda$. The offspring population size is dynamically controlled according to the number of successes [36].
        - An age-based population size control, presented by Arabas, Michalewicz and Mulawka, which introduces the concept of "age" for chromossomes in the evolution process. 
            - The algorithm employed by his work is denominated as Genetic Algorithm with Varying Population Size (GAVaPS). The authors applied this method to find optimal values for test functions. This algorithm does not considerate any type of selection mechanism during the generation as this is replaced with the concept of an aging process for the chromossomes.
            - The algorithm processes a population $P$ and performs a selection of chromossomes, with a selection ratio $p$, without regard to their fitness scores (this step is not considered as a selection step because a fitness evaluation is not made) and an auxiliary population $P'$ is created to insert the future offsprings [37] during the recombination step. The chromossomes are selected with equal probability during the selection process. After selection, genetic operators such as crossover and mutation are applied to the chromossomes.
            - The age-based method of this algorithm consists in assigning a lifetime parameter to the chromossomes applying a aging process to them, i.e., each chromossome will have a fixed lifetime expectancy. In each iteration of the generation, the age of each chromossome is increased by 1 (their age is initially set to 0). After a chromossome exceeds its lifetime, the chromossome will be eliminated from the population. This strategy proposes to reduce computational costs provided by standard selection methods by providing easier calculations [37].
            - Lifetime calculation must obey a set of strategies. For one, the individuals with above-average fitness should prevail in the population and to ensure this they must be able to live longer, i.e., having higher lifetime values. In contrast, the individuals with below-average fitness scores should obtain a short lifetime. A set of measures needed for the lifetime calculations are as follows:
                - Average fitness in the current population ($AvgFit$)
                - Maximum fitness in the current population ($MaxFit$)
                - Minimum fitness in the current population ($MinFit$)
                - Maximal fitness value found during the generation ($AbsFitMax$)
                - Minimal fitness value found during the generation ($AbsFitMin$)
                - Maximal lifetime value ($MaxLT$ - parameter of the algorithm)
                - Minimal lifetime value ($MinLT$ - parameter of the algorithm)
            - Lifetime was applied to the chromossomes according to three different calculations for maximization:
                ![](https://hackmd.io/_uploads/r1Rmhvsnn.png)
                
                - Proportional allocation
                    - Based upon a roulette-wheel selection technique where the lifetime of an individual is proportional to its fitness score. Similar to the roulette wheel technique where the probability of selection is proportional to the fitness score.
                    - However it does not take into account an overall sense of objective. It only envisions the information of the current population while not using information of best seen values. 
                    ![](https://hackmd.io/_uploads/r1dAFDi3n.png)
                - Linear allocation
                    - This technique analyses best seen values for the lifetime calculation during the generation process. It takes an approach on a more global aspect.
                    - However as it delves on global values, it does not evaluate properly values within the population undergoing the generation process. One example of this is related to the average fitness score for the population not being evaluated. In cases where a larger group of individuals have their lifetime equal or almost equal to the best seen lifetime value this can result in allocating long lifetime values for the chromossomes inducing a increasing in size of the population.
                    ![](https://hackmd.io/_uploads/ryVl5Dj22.png)
                - Bi-linear allocation
                    - Makes a compromise between the proportional and linear allocation methods by evaluating current and best seen fitness values.
                    ![](https://hackmd.io/_uploads/Hk1W5Pi33.png)
            - A total of 20 execution runs were executed with different estabilished parameters and in summary the linear strategy proved to be the best performing one but with higher computational cost, the bi-linear strategy was the cheapest method but didn't have a performance as good as the linear method and finally the proportional technique had medium performance and medium cost when considering all three different applications.
    
        - Population Resizing on Fitness Improvement GA (PRoFIGA)
            - A population size mechanism introduced by Eiben, Marchiori and Valkó [39] (PRoFIGA) which consists in controlling the population size during the generation process using three different methods according to the best fitness value improvement. 
            - The mechanism becomes more biased towards exploration when the best fitness of the population improves [39]. When the best fitness is not improved, the population size will gradually decrease however in stagnation periods (where best fitness has not been improved over $x$ fitness evaluations) the population grows in size.
            - Their genetic algorithm uses a 2-tournament selection technique with a 2-point crossover and a bit-flip mutation with replacement of parents after offspring generation.
            - Growing or decreasing the population size happens according two three different conditions:
                1. **Improvement of the best fitness**: the population size increases in proportion to the fitness improvement and the remaining number of evaluations until the maximum evaluations. The formula that enables the population size increase by a $X$ factor is presented:
                ![](https://hackmd.io/_uploads/H1Bbbaga3.png) [39]
        
                $increaseFactor$ - external parameter within the interval $(0,1)$
        
                $maxEvalNum$ - maximum number of fitness evaluations
        
                $currEvalNum$ - current fitness evaluation number
        
                $maxFitness_{new}$ - best fitness value in the current population
        
                $maxFitness_{old}$ - best fitness value in the preceding generation
        
                $initMaxFitness$ - best fitness value in the current generation
        
                2. **Stagnation periods**: in situations where the best fitness score of the population does not increase during $x$ fitness evaluations, the population size is increased by a $Y$ factor. The technique to increase the population size is the same as mentioned in the **improvement of the best finess** condition using the same growth rate $X$ as previously showed [39];
                3. **Lack of best fitness improvement**: lack of improvement (steps 1. and 2. are not executed) makes the population size decrease by a factor $Z$ where a little percentage of the current population size is used $(1-5\%)$ [39]. Regarding the decreasing of population, this method does not decrease the size further if a minimal population size is reached.
                - PRoFIGA was evaluated in comparison with the GAVaPS, Adaptive population size (APGA) [38] and three variants of Random Variation of Population Size (RVPS) genetic algorithms. The results suggest that PRoFIGA was the second best genetic algorithm with 20% lower fitness evaluations across 100 runs on a multi-modal problem being the APGA the best one in terms of efficiency and speed being followed by PRoFIGA.

        - A lot of works took the age-based process from the work done by Arabas, Michalewicz and Mulawka [37] to perform a control of the population size during the evolution process. Each of the following works use this aging process but with different applied methodologies and purposes. In the following points, two major works, that use the notion of lifetime for the chromossomes, are analysed:
            - A different approach by Bäck, Eiben and Vaart [38], addresses the same issue about managing population size through an adaptive solution. In their work, they nominated the aging process as "Remaining Lifetime" (**RLT**) where in each chromossome a lifetime value is allocated decreasing by one every iteration. The RLT value is maintained for the fittest member and a individual is removed from the population when RLT reaches zero.
                - As for lifetime calculation, they also use the bi-linear allocation from the work [37] but adapted to their test functions.
                ![](https://hackmd.io/_uploads/ry46Ojihn.png)
                
                - The authors tested this adaptive population size control in a algorithm called APGA. They employed other algorithms with variants for different parameters (crossover and mutation specifically).
            
            - An Adaptive Population Pool Size Based Genetic Algorithm (**APOGA**) was proposed by Rajakumar and George [40], where they delve upon the problem of population size control using a algorithm, created by them, that enables the population increase ou decrease regarding their algorithm performance.
                - The work tries to tackle the major problems associated with works on the field like the RLT aging process [38] and the PRoFIGA technique [39]. The former only discusses a decreasing method to control population size disregarding any type of increase method. This can be detrimental in some situations where the search for better solutions can be possible the more individuals the population possesses. The latter does indeed discuss an increasing and decreasing method, however the method employed by the authors to decrease the population size has low significance when compared to the increasing portion (decreasing $(1 - 5\%)$ of the population size does not make a huge impact).
                - Their problem uses a fitness function focused in a minimization problem which means the lowest fitness score obtained, the better are the results.
                - They also use the RLT aging process proposed by Bäck, Eiben and Vaart [38] to estabilish a lifetime to the individuals in the population. There is no major differences despite some nomenclature in the used formula.
                ![](https://hackmd.io/_uploads/r1HnTkbah.png) [38]
                
                ![](https://hackmd.io/_uploads/BJGCT1-63.png) [38]

                - As for genetic operators, they select half of the population based on best fitness values where each pair of parents will generate a offspring by applying single crossover and Gaussian random mutation operations.
                - The major part of their work relies on their population resizing method. They use two different strategies to control the population size being one targeted to increase the population and the other to decrease the population.
                    - **Population increase - best fitness improvement**: similar to the strategy shown in [38], the authors grow the population when the best fitness score is improved in each iteration or if the best fitness is not improved for a $x$ amount of iterations. The formula used to increase the population size is shown:
                                                                                ![](https://hackmd.io/_uploads/HJcFXl-63.png)

                    $a$ - random value from the interval $(0,1)$
                    $G$ - growth size
                    $F^{newbest}$ - best fitness value of the current iteration
                    $F^{oldbest}$ - best fitness value of previous iteration
                    $F^{initialbest}$ - initial best fitness value
                    $I^{max}$ - maximum number of iterations
                    $I$ - current iteration number
                    
                    - **Population decrease - RLT**: to decrease population size the aging process RLT is applied to this work. The aging method is essentially the same as described in [38] where initially each chromossome is assigned a lifetime whose values is decreased by 1 for each iteration during the generation process. Those chromossomes who have a RLT value of 0 are removed from the population. Additionally if neither of the above conditions are met (no improvement of the best fitness in each iteration and for some $x$ amount of iterations), then a $D\%$ of chromossomes are removed from the population, being this a parameter from the interval $(0,1$). There is only one major difference to their application. The authors do not decrease the lifetime of the best chromossome, i.e, the chromossome who has the best fitness score of the population. The RLT verification happens each iteration of the generation process.

                  ![](https://hackmd.io/_uploads/S1seBbMp3.png) [40]

                - This algorithm was tested against a standard genetic algorithm form a unimodal test function during 500 iterations of both approaches for 5 runs with different solution spaces (in this case for 5 different populations). The APOGA approach demonstrated a huge minimization (lower fitness values) in the end of the interations compared to the standard GA revealing that the population pool size is starting to turn adaptive to the fitness function [40].
                    
- Selection
    - Another major important part of the genetic algorithm is the selection process where appropriate parents are chosen in order to enable the generation of better individuals to include diversity into the population.
    - Premature convergence is a serious problem to consider when selecting parents for reproduction. A local optimum can be found rather quickly during the search process however this only allows founding a sub-optimal solution. This is not desired as the optimal solution (global optimum) must be always sought after. Convergence of the population is often associated with the concept of **selection pressure**.
    - Selection pressure can be defined as to how much the better individuals in the population are considered to be selected. A higher selection pressure means that the fittest individuals are favoured on the contrary a lower selection pressure means that these fittest individuals are not favoured as much. A higher selection pressure results in a higher convergence of the population to a optimal solution while a lower selection pressure results in a low convergence. This concept is what drives the genetic algorithm to find optimal solutions (for the given problem domain) during the search process. 
    - Ideally, selection pressure should be low in the early stages of a genetic algorithm in order to allow a wide search of the search space to evaluate almost all possible solutions (incentive on a exploration phase). As the generation process advances, selection pressure should gradually be higher to find the global optimum space and then proceed the search solely on that solution vicinity to decide on the best solution possible (incentive on a exploitation phase). The convergence should be lower in the beginning  phase and should rise towards to end to find the optimal solution [41].
    - Some authors applied different techniques to enable the control of selection pressure during the generation process in genetic algorithms:
        - Pham and Castellani [42] proposed an adaptive selection routine for evolutionary algorithms that consists in applying a stochastic noise to determine which individuals should mate (should be selected for reproduction). This work defends the importance of selection pressure and how its control can be favorable towards the optimization of the selection process in evolutionary algorithms. The authors divided their efforts in two main stages:
            1. **Selection pressure adjustment**: as selection pressure is an important factor regarding population convergence for a optimal solution, the authors decided to control the selection pressure by adding individuals to two different structures and mating them between those two. Initially, a noise addition procedure was made to introduce some a random positive perturbation to the fitness of each individual [42]. The result of the mentioned operation is called as **mating chance** by the authors being calculated in the following way:
                ![](https://hackmd.io/_uploads/rkuese86h.png)
                
                - $f(i)$ - fitness of the individual $i$
                - $rand$ - random number in the interval $(0,1)$
                - $f_{max}$ - maximum fitness of the population
                - $\mu_{f}$ - average fitness of the population

                For this step, mating chance is calculated for all individuals and used two times. The first time is to rank the individuals according to their mating chance and the best half is inserted into structure A (temporary list). The second time the same procedure is done but the population is ranked in descending order of mating change and the top half is inserted into structure B (also a temporary list).
                
                This process is made to control the selection pressure as the search goes on in the genetic algorithm. In early stages, the average fitness is likely to be low and if the population has individuals with high values of fitness compared to others the fitness variance will be high ($f_{max}-\mu_{f}$ will be large). Adding a noise perturbation, as exemplified, can help to reduce these large variances and consecutively reduce the supremacy of better individuals in early low average fitness populations and aid the search to continue which consecutively promotes a soft selection pressure at the beginning of the search. Over the iterations of the algorithm, the population will converge, fitness values will be higher and the difference $f_{max}-\mu_{f}$ will be even smaller leading the search to a exploitative phase. This rationale meets the reasoning behind the proper selection pressure evolution [41]. Additionally, the authors also limit the selection of the same solution by a maximum of two times per list which also helps to prevent early convergence.
            2. **Mating individuals**: after the individuals are organized in both structures, a mating operation will start where the first individual from structure A mates with the last individual from the structure B, the second individual from structure A mates with the second to last individual from structure B and so forth until both structures are fully mated [42]. This enables the reproduction between high and low fitness individuals, gradually improving the fitness over the course of the evolution process in the genetic algorithm.
            - This technique was compared to other standard selection methods used in genetic algorithms such as proportional selection, tournament selection and fitness ranking in three diferent phases of the evolution process (early, mid and convergence evolution stages). A number of selections for each individual in the population was registered where the proposed method was compared individualy with each standard method. The results suggest that the proposed selection method agrees with the reasoning behind the proper selection pressure evolution [41] where the selection pressure gradually increases throughout the evolution process. As for the standard methods, the selection pressure generally showed that it would decrease throughout the evolution process which is not the ideal as it can lead to premature convergence.
        - Another method to improve the selection of parents during the execution of the genetic algorithm was proposed by Jebari [43] where different standard selection methods are selected, properly evaluated between generations and the best result from the best selection method is taken into account during the generation process. Their technique can be explained in the following steps:
            1. **Selection of standard selection methods**: a set of standard selection methods in genetic algorithms was chosen in order to be evaluated during the execution of the algorithm. These selection methods include the roulette wheel selection, stochastic universal sampling, linear and exponential ranking selection, tournament selection and truncation selection
            2. **Evaluation of standard selection methods**: for each generation during the execution of the algorithm, every selection method of the set was applied and results were taken. Only the best performing selection method was taken into account for the generation, i.e., the selection that provided the best individual was the one that made impact for the recombination phase. The selected individual was evaluated with two objective criteria, one that measures the quality of solution regarding the fitness value (best fitness value is chosen) and the other to measure the diversity of the individual according to the rest of the population, where the most diverse individual from the population is selected [43].

            - The work analysed this selection technique against each standard selection method that was selected. The comparison occurred using three different runs with three different test functions. The results suggest that the new selection procedure obtained more optimal parents than the standard selection methods proving to be better a selection optimal parents. However, this work wastes a lot of computational resources as each standard selection method is evaluated in each generation to decide the best one to use which can increase execution times for the genetic algorithm.
        
- Crossover and Mutation 
    - Crossover and mutation are necessary operations to introduce new variety into the population. Exchange of information between selected parents happens to obtain a new individual (crossover) being it followed by a information change operation within the chromossome genes to introduce even more diversity into the individual (mutation) where both operations happen in this specific order (these do not happen simultaneously). Both of these operations have a probability to occur during the run of the genetic algorithm and are a vital aspect for the efficiency of the algorithm as it aids the search process to find optimal solutions. Without these operations, only parents would be selected and no new individual would be created and consecutively no solution would be found.
    - These operators can have a huge impact for the search process according to their probability of execution: 
        - **Crossover rate**: higher crossover rates can introduce new individuals into the population more frequently however if it is too high, individuals with high-fitness (that were selected previously) can lose good genes with the crossover operation [47]. Lower crossover rates do not create new individuals and the search might stagnate early on.
        - **Mutation rate**: higher mutation rate introduces new information into the individuals however if it is too high it induces a random search [47] potentially creating worse individuals and affecting existing individuals with high fitness. Lower mutation rates mantain the genetic information within the individual. In situations where the individual, after the crossover operation, did not present any major improvement in fitness, lower mutation rate will mantain that genetic information and consecutively not add any type of variety into the population. 
    - Several works delve upon the dynamic variance of crossover rates for genetic algorithms such as:
        - The work proposed by Hassanat, Almohammadi, Alkafaween, Abunawas, Hammouri and Prasath [44] presents two deterministic parameter control techniques that focus on the variance of crossover and mutation rates during the execution of a standard genetic algorithm. The authors created these two methods and compare them to other two standard parameter tuning techniques being fifty-fifty crossover/mutation ratios and the 0.9 crossover and 0.03 mutation ratios.
            - The authors applied this under the Travelling Salesman Problem (TSP) that consists in finding the closest path through a number of node that have the same star and endpoints where each node is connected and used only once [44]. 
            - The authors proposed two different deterministic parameter tuning techniques:
                - The first technique is called Dynamic Increasing of Low Mutation/Decreasing of High Crossover (ILM/DHC). This is a deterministic method where a increase/decrease rule is applied to the crossover and mutation rates. The objective behind this technique is to gradually increase mutation rate (starts at 0% and increases linearly to 100%) and gradually decrease the crossover rate (starts at 100% and decreases linearly to 0%) during the execution of the genetic algorithm. The equations behind the increase of mutation rate and decrease of crossover rate are presented respectively:

                    ![](https://hackmd.io/_uploads/HkufxGIC3.png) [44]

                    ![](https://hackmd.io/_uploads/Skx0gfIAh.png) [44]
                
                    - $MR$ - mutation rate
                    - $CR$ - crossover rate
                    - $G_n$ - maximum number of generations (fixed parameter)
                    - $LG$ - number of generation level (current generation)
                    - $popsize$ - population size
                    - $M$ - number of chromossomes that need to be mutated
                    - $C$ - number of chromossomes that need to be used for crossover

                    - In each generation level, both mutation and crossover rates will linearly increase and decrease, respectively, with the use of the equations above.
        
                - The second technique is called Dynamic Decreasing of High Mutation Rate/Increasing of Low Crossover Rate (DHM/ILC) and is the opposite of the first technique. In this case, crossover rate gradually increases (starts at 0% and increases linearly to 100%) and mutation rate gradually decreases (starts at 100% and decreases linearly to 0%). The equations are exchanged between both operators:
            
                    ![](https://hackmd.io/_uploads/ByrlzGIR2.png) [44]
                
                    ![](https://hackmd.io/_uploads/ByvWff8R2.png) [44]

            - As for the standard methods, these are deterministic tuning parameters with fixed parameter values during the run of the genetic algorithm. The authors compared their dynamic approach to the following standard parameter tuning methods:
                - **Fifty-fifty crossover/mutation ratios**: during all generations of the genetic algorithm, all chromossomes have a 50% chance to undergo a crossover process and a 50% chance to undergo a mutation process. These parameter values are chosen before the run of the algorithm.
                - **0.9 crossover and 0.03 mutation ratios**: during all generations of the genetic algorithm, all chromossomes have a 90% chance to undergo a crossover process and a 3% chance to undergo a mutation process. These ratios were based upon other works on the area of research regarding parameter optimization. These parameters are also chosen before the run of the algorithm.
            
            - Six different set of experiments were executed with varying population sizes (small with 25 and 50, moderate with 100 and 200 and large with 300 and 400) on the two proposed dynamic approaches and the two standard deterministic parameter tuning techniques. 
            - The authors used roulette wheel selection, one-point crossover, exchange mutation and the termination criteria was set on a fixed number of generations reached (maximum number of generations).
            - In each set the genetic algorithm was executed 10 times. The results obtained concluded that: 
            - For the **small population sizes**, ILM/DHC proved to be the best technique as it showed the lowest number of convergences for the solution in relation to the other three techniques. This means that the technique was quicker that the others in finding a optimal solution. This is often explained that in smaller population sizes, a higher mutation rate is often more beneficial than a higher crossover rate as it introduces more variety to a small population [44] (which may not have good individuals as the population is smaller).
            - For the **moderate population sizes**, ILM/DHC proved to be the best technique for population size 150 and DHM/ILC proved to be best for population size 200. The second result can be explained according to the growing size of the populations. A higher population size can include individuals with high fitness and a higher mutation rate in the beginning is not needed for this situation, on the contrary a higher crossover rate is needed to generate better offspring from large population sizes [44]. As the generations go on, then the focus can go into mutation as the majority of individuals should have high fitness in the population and crossover is way less impactul than applying a mutation on the later generation levels.
            - For the **large population sizes**, DHM/ILC proved to be the best in relation to the other techniques because of the evidence showed in the last sections as the increase of the population size indicates a better performance for increasing crossover and decreasing mutation rates.
            - The standard methods in all experiments did not have better results than the dynamic approaches, proving that even the deterministic approaches present better results than the conventional values in standard parameter tuning techniques.
        
        - Another work, developed by Vannuci and Colla [48], introduces an adaptive approach to adjust the crossover and mutation rates of a standard genetic algorithm according to feedback obtained from the search process. This is applied with the use of a Fuzzy Inference System (FIS), the strategy to apply this according another factors is named as Fuzzy Adaptive Recombination Strategy (FARS). The authors propose the control of a set of indicators that assess the search performance through the FIS and according to these a tuning process happens in the recombination and mutation levels (crossover and mutation rates) [48].
            - The objective of FARS is to iteratively adjust crossover and mutation rates of the genetic algorithms according to two main factors:
                - **Trend of fitness**: the candidate solutions in a population are evaluated, in each generation, in order to perceive if the algorithm tends to obtain better or worse levels of fitness throughout the generations. The current generation is evaluated using the slope of the average fitness of the 25% fittest individuals. A weighted average of slope in $M_t$ previous generations is considered to prevent fluctuations  [48]. The trend is evaluated in each generation to verify if the levels of fitness are increasing or decreasing. The value is within the range $[-90,90]$.
                - **Search temporal phase**: crossover and mutation rates are adjusted according to the current temporal phase of the algorithm (if it is in an initial or an advanced state of execution). Low mutation rates and high crossover rates are recommended for earlier stages of the algorithm as the exploration phase is more emphasized. As the genetic algorithm reaches its final stages a exploitation phase is recommended, with higher mutation rates and lower crossover rates in order to create better individuals within a pool that already has a great amount of high fitness individuals. The value is within the range $[0,1]$ representing the progress obtained according to max number of generations.
            - Stability is another factor introduced by the authors, it is determined by the number of sign changes of the derivative of the average fitness of the 25% fitness individuals in $M_s$ previous generations [48]. The value is within the range $[0,1]$ where 0 refers to a stable behavior and 1 to a unstable behavior.
            - The authors use two different FIS systems to optimize the crossover and mutation rates. They used one of them to only optimize the mutation rate and the other one to only optimize crossover rate. The inputs used for the FIS that optimizes mutation rate are phase and trend. The inputs used for FIS that optimizes the crossover rate also takes the phase and trend with an additional stability variable. These inputs are then converted into fuzzy sets (fuzzifier process) being this divided into the following sets:
                - **Phase**: initial and advanced 
                - **Trend**: negative, neutral and positive
                - **Stability**: stable and unstable
                - **Rates variation**: negative, constant and positive
            :::warning
            The fuzzy sets refer to the possible values that each variable can have for example the fuzzy set for phase variable is initial and advanced.
            :::
            - Each of the fuzzy sets have a specified membership function to define the degree of membership of an input value to a certain fuzzy set within the range $[0,1]$. 
            
            :::warning
            The membership function used by the authors is not presented however by the looks of the graphs it seems like they used a triangular membership function for the phase and rate variation variables. The trend variable seems to be converted with a trapezoidal membership function. The stability variable seems to be converted into a fuzzy set with a s-function. 
            :::
            - A set of rules, defined by the authors, have been applied for minimization problems, which means the factor trend is desired to have the lowest value possible. With the use of inference engine of a FIS, the matching degree of a input variable with correspondence to a rule is determined and a decision is made for which rule to be executed. In this case a fuzzy output set is created after deciding the rules to apply to the inputs.
                ![](https://hackmd.io/_uploads/HJtW_moJ6.png)

            - A defuzzifier process occurs in the fuzzy output sets to be converted into a output value that can be used externaly to the FIS. The authors then obtain two output values $\delta_c$ (crossover) and $\delta_m$ (mutation) that are going to be added into the existing crossover and mutation rates as $\hat{r}_c = r_c + \delta_c$ and $\hat{r}_m = r_m + \delta_m$, where $\hat{r}_c$ represents the new crossover rate and $\hat{r}_m$ represents the new mutation rate to apply in the current generation.

            :::warning
            ![](https://hackmd.io/_uploads/HJhbWkyla.png)
            :::
            
            - The strategy applied by the authors was compared to other crossover and mutation optimization techniques such as:
                - **Constant rates**: fixed rates for crossover and mutation across all generations. These rates were $r_c = 0.6$ for crossover and $r_m = 0.2$ for mutation.
                - **Alternate rates**: two different configuration rates for crossover and mutation that alternate between a specified number of generations. These rates were $r_c = 0.8$ and $r_c = 0.4$ for crossover and $r_m = 0.1$ and $r_m = 0.3$ for mutation.
                - **Mutation rate optimization**: a mutation rate control proposed by Back and Schutz [49], where the mutation rate is adjusted across the generations. For this method, the crossover rate remains fixed during the execution of the genetic algorithm.

            - The experiments were applied for 100 runs of the genetic algorithm with the FARS, constant, alternate and mutation rate optimization methods for minimization problems which include a set of minimization functions. The authors verified that for simpler minimization problems, all the methods converged for a solution but FARS revelead itself to be the fastest to converge and ultimately reducing the computational time. For more moderate problems, FARS and mutation rate optimization methods were similar in terms of convergences however the former obtained a quicker convergence. For more complex problems, FARS proved to be the best method to achieve convergence as its crossover and mutation rate adjustment avoids local minima, which was the main reason explained by the authors about the lack of convergence of the other methods as these were affected by the presence of local minima.
        
        - Self-adaptive methods have also been applied and some examples can be found in the works of Bäck, Eiben and Vaart [33], Bäck and Schütz [50], Breukelaar and Bäck [51]. 
            - A new self-adaptive method for crossover rates was proposed and a self-adaptive method for mutation rates was implemented by the authors [33]. The methods implemented are explained according different objectives:
                - **Crossover evolution**: a self-adaptive crossover method where a initial crossover rate was randomly chosen in the interval $[0,1]$ and encoded into the individuals representation scheme. After an individual is selected in the selection process, a random number $r$ below 1 is compared with the encoded crossover rate $p_c$ of the individual. 
                    - If $r$ is lower than the $p_c$ then the individual is ready for the crossover operation. If both parents have their $p_c$ higher than $r$ then a new child is created by crossover being it followed by the next in order set of genetic operators (mutation and addition to population) [33].
                    - If $r$ is higher than the $p_c$ then the individual will not undergo crossover and will suffer a mutation to create one child. This child will undergo a mutation process and survivor selection immediately [33].
                    - If both parents do not have their $p_c$ value higher than $r$ then two children are created with mutation only. If one parent has its $p_c$ value higher than $r$ and the other parent does not, then the latter suffers mutation to create a child which is inserted into the population immediately [33]. The other parent is kept and in the next selection process only one parent is picked.
                        :::warning
                        In this case, for implementation each condition must be verified and the crossover method (exchange between methods of test cases) can occur as a normal crossover operation despite the authors have chosen to apply the uniform crossover operator. 
                        :::
                        
                - **Mutation evolution**: the mutation method applied by the Bäck, Eiben and Vaart [33] is an already existing method proposed by Breukelaar and Bäck [50]. This method consists in adapting mutation rates that are encoded in the individuals. A initial mutation rate is encoded in the individuals representation being its value chosen randomly in the interval $[0.001, 0.25]$, this mutation rate will undergo mutation operations during the generations. The rate is mutated in the following way:
                    - The encoded mutation rate is mutated using the formula 
                    
                        ![](https://hackmd.io/_uploads/BJDnM7mgp.png) [51]
                        
                        - $p$ - encoded mutation rate
                        - $\gamma$ - constant
                        - $N(0,1)$ - random value from a normal distribution with mean 0 and standard deviation 1 
                        - $p´$ - adjusted mutation rate

                    - The $\gamma$ is a constant to control the convergence speed of the algorithm being usually set to $0.22$ [51]. This formula adjusts the mutation rate present in the representation scheme of each chromossome that was selected during the parent selection process. For the next set of generations, the probability of mutation for a certain individual is obtained through their representation and updated accordingly.
                    
                        :::warning
                        The same can be said for the mutation process regarding its application, the way to apply the mutation can occur as a normal mutation operation despite the authors particulary chose to apply a bit flip mutation operator.
                        :::
                        
            - As all the above mentioned works use the same mutation method and the work of Bäck, Eiben and Vaart [33], only the results for this work are considered. The authors made the comparison of a traditional genetic algorithm (TGA) with a genetic algorithm with self-adaptive mutation only (SAMGA), a genetic algorithm with self-adaptive crossover only (SAXGA) and a genetic algorithm that uses both self-adaptive methods (SAMXPGA). They also considered a genetic algorithm with adaptive population size but for this analysis of the authors results, only the self-adaptive and traditional genetic algorithms are considered. The results observed reveal that:
                - SAMGA and SAXGA revealed to be worse in terms of execution time than TGA when trying to find the optimum for test functions. One reason the authors gave for this situation is that the methods focused too much in optimizing their respective parameters instead of finding the overall objective of the search this being the optimum.
                - SAMXPGA is the best genetic algorithm when compared to TGA, SAMGA and SAXGA where obtained the fastest execution times and achieved the optimum faster. One reason the authors present to explain this behavior is the adaptive population size greatly helps with the search process, reducing the time when evaluating individuals while only mantaing the fittest individuals for a longer time during the generations.

:::warning
Michael Marcozzi - FIS
:::

:::info
According the work of dynamic crossover and mutation rates for the deterministic parameter control methods, $n$ that stands for chromossome size in time complexity calculation can be the number of test cases in a test suite which represents a solution.
:::


###### Genetic Algorithm Quality (IMPORTANTE - POSSUI METRICAS)

...

:::warning
For minimization problems, majority of times the conditions need to be only reversed to convert to maximization problems according to the logic of genetic algorithms adjustment during generations. One example of this is reversing conditions for fitness evaluation with average fitness to apply a different equation.

If results are not to be expected see equations again when applying the generation process. Reversing conditions might not be enough when converting minimization to maximization.
:::

:::info

**Additional info**

Local vs Global Optima

Local optima and global optima are concepts related to optimization problems, including those solved by hill climbers and genetic algorithms. Let's understand the distinctions:

Local Optima:

Local optima refer to solutions that are optimal within a specific region of the search space. These solutions may be the best in their local vicinity, but they are not necessarily the best solution globally across the entire search space.
In the context of optimization algorithms like hill climbers, when the algorithm starts from a specific point in the search space and iteratively explores its neighboring solutions, it may converge to a local optimum that is the best in that particular neighborhood.
The drawback of local optima is that they may prevent the algorithm from finding the overall best solution if that solution exists in a different region of the search space.
Global Optima:

Global optima, on the other hand, refer to the best possible solution across the entire search space. These are the optimal solutions that one would ideally like to find when solving an optimization problem.
In the context of optimization algorithms like genetic algorithms, which use a population of solutions and evolutionary processes, the algorithm aims to explore different regions of the search space and converge to the global optimum, i.e., the best possible solution overall.
Finding the global optimum can be challenging, especially in complex and high-dimensional search spaces, as the algorithm needs to overcome local optima to reach the best possible solution.

![](https://hackmd.io/_uploads/ByWBycrFh.png) 

Distinction in Hill Climber and Genetic Algorithm:

Hill Climber:

Hill climbers are local search algorithms that start from a specific point in the search space and iteratively move to neighboring solutions that improve the objective function.
They are prone to getting stuck in local optima, as they focus on exploring the nearby solutions and may not venture far enough to find the global optimum.
Hill climbers are computationally efficient and work well for some simple optimization problems with a well-defined landscape.
Genetic Algorithm:

Genetic algorithms, on the other hand, are population-based metaheuristics that maintain a population of solutions and apply genetic operators (crossover, mutation, selection) to evolve the population over generations.
By maintaining a diverse population and applying genetic operators, genetic algorithms are more capable of exploring different regions of the search space, which helps them avoid getting trapped in local optima and increases the chance of finding the global optimum.
However, genetic algorithms might require more computational resources and can be slower compared to hill climbers, especially for problems with a large search space.
In summary, hill climbers are more prone to finding local optima due to their localized search strategy, while genetic algorithms are better suited to explore the search space globally and have a higher chance of finding global optima. However, genetic algorithms might require more computational resources and be computationally expensive compared to hill climbers, making the choice of optimization algorithm dependent on the specific problem characteristics and available resources.
:::